from pyspark.sql import SparkSession
spark = SparkSession.builder.master("local").appName("Name1").getOrCreate()
print(spark)
print(type(spark))
rdd = spark.sparkContext.parallelize([1,2,3,4, 6,7,8,8, 9,45,4322])
print(rdd.count())
